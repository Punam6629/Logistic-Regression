{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V46rIlJQFvu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic** **Regression**"
      ],
      "metadata": {
        "id": "D3hGplk6QaHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "- Logistic Regression is a statistical and machine learning algorithm used for binary classification problems — where the output (target variable) is categorical, typically taking values like 0 or 1, yes or no, true or false, etc.\n",
        "-  Logistic Regression: Overview\n",
        "- Purpose: Predicts the probability that a given input belongs to a particular class.\n",
        "\n",
        "- Output: Values between 0 and 1, representing the probability of class membership.\n",
        "\n",
        "- Function Used: Uses the sigmoid (logistic) function to map linear outputs to a probability.\n",
        "\n",
        "- Sigmoid Function Formula:\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "σ(z)=\n",
        "1+e\n",
        "−z\n",
        "\n",
        "- 1\n",
        "​\n",
        "\n",
        "- Where\n",
        "𝑧\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "…\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "z=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +b\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +…+b\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "- Linear Regression: Overview\n",
        "Purpose: Predicts a continuous numerical output.\n",
        "\n",
        "- Output: Any real number (from\n",
        "−\n",
        "∞\n",
        "−∞ to\n",
        "+\n",
        "∞\n",
        "+∞)\n",
        "\n",
        "- Function Used: Simple linear equation.\n",
        "\n",
        "- Linear Regression Formula:\n",
        "𝑦\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "…\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +b\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +…+b\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "2. What is the mathematical equation of Logistic Regression.\n",
        "- The mathematical equation of Logistic Regression models the probability that a given input belongs to the positive class (typically labeled as 1) using the sigmoid function.\n",
        "\n",
        "- 1. Linear Combination (like in Linear Regression):\n",
        "𝑧\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "…\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "=\n",
        "𝑏\n",
        "𝑇\n",
        "𝑥\n",
        "z=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +b\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +…+b\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " =b\n",
        "T\n",
        " x\n",
        "- Where:\n",
        "\n",
        "- 𝑏\n",
        "0\n",
        "b\n",
        "0\n",
        "​\n",
        "  = intercept (bias term),\n",
        "\n",
        "𝑏\n",
        "1\n",
        ",\n",
        "𝑏\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝑏\n",
        "𝑛\n",
        "b\n",
        "1\n",
        "​\n",
        " ,b\n",
        "2\n",
        "​\n",
        " ,...,b\n",
        "n\n",
        "​\n",
        "  = model coefficients,\n",
        "\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        "x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,...,x\n",
        "n\n",
        "​\n",
        "  = input features.\n",
        "- 2. Sigmoid (Logistic) Function:\n",
        "- The output probability is passed through the sigmoid function:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        ")\n",
        "P(y=1∣x)=σ(z)=\n",
        "1+e\n",
        "−z\n",
        "\n",
        "1\n",
        "​\n",
        " =\n",
        "1+e\n",
        "−(b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +⋯+b\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "- Final Equation of Logistic Regression:\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        ")\n",
        "P(y=1∣x)=\n",
        "1+e\n",
        "−(b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +b\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+b\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "- This represents the probability that the output\n",
        "𝑦\n",
        "y is 1 given the input\n",
        "𝑥\n",
        "x.\n",
        "- 4. Classification Rule (Decision Boundary):\n",
        "Once the probability is computed, we typically classify as:\n",
        "\n",
        "𝑦\n",
        "^\n",
        "=\n",
        "{\n",
        "1\n",
        "if\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "≥\n",
        "0.5\n",
        "0\n",
        "otherwise\n",
        "y\n",
        "^\n",
        "​\n",
        " ={\n",
        "1\n",
        "0\n",
        "​\n",
        "  \n",
        "if P(y=1∣x)≥0.5\n",
        "otherwise\n",
        "​\n",
        "\n",
        "3. Why do we use the Sigmoid function in Logistic Regression.\n",
        "-  Reasons for Using the Sigmoid Function in Logistic Regression:\n",
        "- 1. Probability Interpretation\n",
        "Logistic regression predicts the probability that an instance belongs to class 1.\n",
        "\n",
        "- The sigmoid function outputs values strictly between 0 and 1:\n",
        "\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "σ(z)=\n",
        "1+e\n",
        "−z\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "- This makes it ideal for modeling probabilities.\n",
        "-  2. Smooth, Differentiable Curve\n",
        "- The sigmoid is a smooth, S-shaped (sigmoidal) curve.\n",
        "\n",
        "- This smoothness allows gradient-based optimization methods (like gradient descent) to efficiently compute derivatives and update model weights.\n",
        "\n",
        "- 3. Threshold-Based Decision\n",
        "Once we have a probability from the sigmoid, we can apply a threshold (e.g., 0.5) to make a decision:\n",
        "\n",
        "𝑦\n",
        "^\n",
        "=\n",
        "{\n",
        "1\n",
        "if\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "≥\n",
        "0.5\n",
        "0\n",
        "otherwise\n",
        "y\n",
        "^\n",
        "​\n",
        " ={\n",
        "1\n",
        "0\n",
        "​\n",
        "  \n",
        "if σ(z)≥0.5\n",
        "otherwise\n",
        "​\n",
        "- 4. Bounded Output\n",
        "- Unlike linear regression which can output anything from\n",
        "−\n",
        "∞\n",
        "−∞ to\n",
        "+\n",
        "∞\n",
        "+∞, sigmoid safely restricts outputs between 0 and 1, avoiding nonsensical probabilities (like 1.3 or -0.5).\n",
        "\n",
        " 5. Natural Link to Log-Odds\n",
        "- Logistic regression models the log-odds (logit) of the probability:\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "1\n",
        "−\n",
        "𝑃\n",
        ")\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "…\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "log(\n",
        "1−P\n",
        "P\n",
        "​\n",
        " )=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +…+b\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "- Solving this for\n",
        "𝑃\n",
        "- P gives the sigmoid function\n",
        "4. What is the cost function of Logistic Regression?\n",
        "- Cost Function of Logistic Regression\n",
        "- In Logistic Regression, we use a cost function called the Log Loss or Binary - Cross-Entropy Loss. It measures how well the predicted probabilities match the actual class labels (0 or 1).\n",
        "- Logistic Regression Cost Function (Binary Cross-Entropy):\n",
        "For one training example:\n",
        "\n",
        "- Cost\n",
        "(\n",
        "𝑦\n",
        ",\n",
        "𝑦\n",
        "^\n",
        ")\n",
        "=\n",
        "−\n",
        "[\n",
        "𝑦\n",
        "⋅\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑦\n",
        "^\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        ")\n",
        "⋅\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "^\n",
        ")\n",
        "]\n",
        "Cost(y,\n",
        "y\n",
        "^\n",
        "​\n",
        " )=−[y⋅log(\n",
        "y\n",
        "^\n",
        "​\n",
        " )+(1−y)⋅log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        " )]\n",
        "Where:\n",
        "\n",
        "𝑦\n",
        "∈\n",
        "{\n",
        "0\n",
        ",\n",
        "1\n",
        "}\n",
        "y∈{0,1}: the true label,\n",
        "\n",
        "𝑦\n",
        "^\n",
        "=\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "y\n",
        "^\n",
        "​\n",
        " =σ(z): predicted probability from the sigmoid function.\n",
        "5.  What is Regularization in Logistic Regression? Why is it needed.\n",
        "- What is Regularization in Logistic Regression?\n",
        "- Regularization is a technique used to prevent overfitting by penalizing large coefficients (weights) in the logistic regression model. It adds a penalty term to the cost function to discourage the model from becoming too complex.\n",
        "-  Why is Regularization Needed?\n",
        "- 1. Prevents Overfitting\n",
        "- A model with too many or very large coefficients can fit the training data too well, including noise.\n",
        "\n",
        "- Regularization shrinks these coefficients, leading to a simpler model that generalizes better to unseen data.\n",
        "\n",
        "-  2. Improves Generalization\n",
        "- It forces the model to focus only on the most important features.\n",
        "\n",
        "- Helps avoid high variance and improves performance on the test set.\n",
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "-   Difference Between Lasso, Ridge, and Elastic Net Regression\n",
        "- All three are regularization techniques used to prevent overfitting by adding a penalty to the regression model's cost function. The key difference lies in how the penalty is applied.\n",
        "-  1. Ridge Regression (L2 Regularization)\n",
        "🔸 Penalty:\n",
        "Cost Function\n",
        "=\n",
        "Loss\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        "Cost Function=Loss+λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " θ\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "- Adds the squared magnitude of coefficients to the loss function.\n",
        "\n",
        "🔸 Effects:\n",
        "- Shrinks coefficients towards zero but never exactly zero.\n",
        "\n",
        "- Keeps all features in the model (none are removed).\n",
        "\n",
        "- Good when many features are correlated.\n",
        "\n",
        "🔸 Use Case:\n",
        "- When all features are useful but need to reduce their impact to avoid overfitting.\n",
        "- 2. Lasso Regression (L1 Regularization)\n",
        "🔸 Penalty:\n",
        "Cost Function\n",
        "=\n",
        "Loss\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝜃\n",
        "𝑗\n",
        "∣\n",
        "Cost Function=Loss+λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣θ\n",
        "j\n",
        "​\n",
        " ∣\n",
        "- Adds the absolute value of coefficients.\n",
        "\n",
        "🔸 Effects:\n",
        "- Can shrink some coefficients to exactly zero.\n",
        "\n",
        "- Performs automatic feature selection.\n",
        "\n",
        "- Useful when we expect only a few features to be important.\n",
        "\n",
        "🔸 Use Case:\n",
        "- High-dimensional datasets (many features, some irrelevant).\n",
        "\n",
        "- When feature selection is desired.\n",
        "-  3. Elastic Net Regression (L1 + L2 Regularization)\n",
        "🔸 Penalty:\n",
        "Cost Function\n",
        "=\n",
        "Loss\n",
        "+\n",
        "𝜆\n",
        "1\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝜃\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "𝜆\n",
        "2\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        "Cost Function=Loss+λ\n",
        "1\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣θ\n",
        "j\n",
        "​\n",
        " ∣+λ\n",
        "2\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " θ\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "- Combines both L1 (Lasso) and L2 (Ridge) penalties.\n",
        "\n",
        "🔸 Effects:\n",
        "- Balances between Lasso's feature selection and Ridge's shrinkage.\n",
        "\n",
        "- Useful when you have many correlated features and want sparsity too.\n",
        "\n",
        "🔸 Use Case:\n",
        "- When neither Lasso nor Ridge alone works best.\n",
        "\n",
        "- Often provides better performance than either individually.\n",
        "7. When should we use Elastic Net instead of Lasso or Ridge.\n",
        "-When to use Elastic Net:\n",
        "- Multicollinearity:\n",
        "- Elastic Net is particularly useful when your data has highly correlated features. Lasso can arbitrarily drop one feature in a group of correlated ones, while Elastic Net can handle multicollinearity better by distributing the effect across correlated features.\n",
        "- Feature Selection:\n",
        "- If you need to select a subset of features for a more interpretable model, Elastic Net can perform feature selection by shrinking some coefficients to zero, similar to Lasso.\n",
        "- High Dimensional Data:\n",
        "- When the number of features is much larger than the number of observations, Elastic Net can be more effective than Lasso or Ridge.\n",
        "- Balancing Feature Selection and Regularization:\n",
        "- Elastic Net allows you to control the balance between L1 (Lasso) and L2 (Ridge) penalties, offering a flexible approach to model building.\n",
        "- When to consider other methods:\n",
        "- Ridge Regression:\n",
        "- If you have multicollinearity but don't need feature selection, Ridge Regression is a good choice. It shrinks coefficients but doesn't eliminate them, which can be useful when many features are relevant.\n",
        "- Lasso Regression:\n",
        "- If you need strong feature selection and your data doesn't have high multicollinearity, Lasso can be a good option. However, if you have many correlated features, Lasso might arbitrarily drop one, leading to suboptimal results.\n",
        "- In essence:\n",
        "- Elastic Net = Lasso + Ridge . It's a hybrid approach that combines the benefits of both.\n",
        "- Choose Elastic Net when: You have correlated features, need feature selection, and want a balance between the two.\n",
        "- Consider other methods: If you have multicollinearity but don't need feature selection, use Ridge. If you need strong feature selection and have little multicollinearity, use Lasso.\n",
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "- The regularization parameter (λ) in logistic regression plays a critical role in controlling model complexity and preventing overfitting. It is used in conjunction with regularization techniques, such as L1 (Lasso) or L2 (Ridge) regularization.\n",
        "- What is λ (Lambda)?\n",
        "- λ (lambda) is a hyperparameter that determines the amount of penalty applied to the size of the coefficients (weights) in logistic regression.\n",
        "\n",
        "- It appears in the regularized cost function:\n",
        "9. What are the key assumptions of Logistic Regression?\n",
        "- The key assumptions of Logistic Regression are crucial for ensuring that the model gives reliable and interpretable results. While logistic regression is more flexible than linear regression, it still relies on several assumptions:\n",
        "- Key Assumptions of Logistic Regression\n",
        "\n",
        " | # | **Assumption**                    | **Explanation**                                                                                                           |\n",
        "| - | --------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |\n",
        "| 1 | **Binary or categorical outcome** | The dependent variable should be binary (0/1, True/False), or multinomial in the case of multinomial logistic regression. |\n",
        "| 2 | **Linearity of logit**            | The **log-odds** of the outcome should have a **linear relationship with the independent variables**:                     |\n",
        "10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "- Several algorithms can be used as alternatives to logistic regression for classification tasks, including decision trees, random forests, support vector machines (SVMs), K-nearest neighbors (KNN), and Naive Bayes. Other options include XGBoost, CatBoost, and neural networks, particularly when dealing with complex relationships or large datasets.\n",
        "- Here's a more detailed look at some of these alternatives:\n",
        "1. Decision Trees:\n",
        "- How they work: Decision trees split data based on feature conditions, creating a tree-like structure for classification.\n",
        "- Pros: Can handle non-linear relationships and are relatively easy to interpret.\n",
        "- Cons: Can be prone to overfitting, especially with complex trees.\n",
        "- When to use: When you need a model that is easy to understand and visualize, or when dealing with non-linear relationships between features and the target variable.\n",
        "- 2. Random Forests:\n",
        "- How they work:\n",
        "- An ensemble of multiple decision trees, reducing the variance and improving generalization.\n",
        "- Pros:\n",
        "- Robust to overfitting, handles high dimensionality well, and can capture complex relationships.\n",
        "- Cons:\n",
        "- Can be computationally expensive for very large datasets.\n",
        "- When to use:\n",
        "- When you need a powerful and robust classifier, particularly for complex datasets with many features.  \n",
        "11. What are Classification Evaluation Metrics?\n",
        "- Classification evaluation metrics are used to assess how well a machine learning model performs in assigning instances to predefined categories. These metrics provide insights into the model's accuracy, precision, recall, and overall effectiveness in classifying data.\n",
        "12. How does class imbalance affect Logistic Regression?\n",
        "- Class imbalance in logistic regression can lead to a model that is biased towards the majority class, resulting in poor performance on the minority class. This means the model may predict the majority class more accurately than the minority class, which is especially problematic when the minority class is of greater interest.\n",
        "- Here's a more detailed explanation:\n",
        "- Bias towards the majority class:\n",
        "- Logistic regression aims to maximize the overall accuracy, so when one class has significantly more instances than others, the model tends to prioritize correctly classifying the majority class, even if it means misclassifying a large number of the minority class.\n",
        "- Poor performance on minority class:\n",
        "- Due to the bias, the model may have low precision and recall for the minority class, meaning it will incorrectly classify many instances of the minority class as belonging to the majority class.\n",
        "- Misleading accuracy:\n",
        "- A high overall accuracy score can be misleading in imbalanced datasets, as it might be driven by the high accuracy on the majority class, while the model performs poorly on the minority class.\n",
        "- Impact on real-world applications:\n",
        "- In scenarios like fraud detection or medical diagnosis, where the minority class represents the rare but important events, poor performance on the minority class can have significant consequences.\n",
        "13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "- Hyperparameter tuning in logistic regression is the process of finding the optimal values for the hyperparameters of the logistic regression model to improve its performance. These hyperparameters are not learned during training, unlike the model's coefficients, and need to be set beforehand. Tuning helps to optimize the model's ability to generalize to unseen data and avoid issues like overfitting or underfitting.\n",
        "14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "- In Logistic Regression, solvers are optimization algorithms used to minimize the cost function (typically the log loss). Different solvers have different strengths depending on the dataset size, sparsity, regularization, and convergence speed.\n",
        "\n",
        "- Here are the main solvers supported by sklearn.linear_model.LogisticRegression:\n",
        "- 1. liblinear\n",
        "- Type: Coordinate Descent\n",
        "\n",
        "- Supports: L1 & L2 regularization (binary & one-vs-rest for multiclass)\n",
        "\n",
        "- Best for: Small datasets, L1 regularization\n",
        "\n",
        "- Limitation: Slower on large datasets, does not support multinomial loss\n",
        "- 2. newton-cg\n",
        "- Type: Newton-Raphson method (second-order optimization)\n",
        "\n",
        "- Supports: L2 regularization\n",
        "\n",
        "- Best for: Multinomial logistic regression, larger datasets\n",
        "\n",
        "- Limitation: Doesn’t support L1 penalty\n",
        "\n",
        "-  3. lbfgs (default in scikit-learn)\n",
        "- Type: Quasi-Newton method\n",
        "\n",
        "- Supports: L2 regularization\n",
        "\n",
        "- Best for: Multiclass classification (efficient for large datasets)\n",
        "\n",
        "- Limitation: Doesn’t support L1 penalty\n",
        "- 4. sag (Stochastic Average Gradient)\n",
        "- Type: Stochastic optimization\n",
        "\n",
        "- Supports: L2 regularization\n",
        "\n",
        "- Best for: Large datasets (especially with many samples)\n",
        "\n",
        "- Limitation: Only effective when data is scaled; doesn’t support L1 penalty\n",
        "\n",
        "-  5. saga\n",
        "- Type: Stochastic Average Gradient (improved version of sag)\n",
        "\n",
        "- Supports: Both L1 and L2, and Elastic Net regularization\n",
        "\n",
        "- Best for: Large-scale datasets, sparse data, and when using Elastic Net\n",
        "\n",
        "- Strength: Supports all penalties and multinomial loss\n",
        "15.  How is Logistic Regression extended for multiclass classification?\n",
        "- Logistic Regression is naturally a binary classifier, but it can be extended to handle multiclass classification using two main strategies:\n",
        "\n",
        "-  1. One-vs-Rest (OvR) / One-vs-All (OvA)\n",
        "-  How it works:\n",
        "- For K classes, train K binary classifiers.\n",
        "\n",
        "- Each classifier learns to distinguish one class vs. all others.\n",
        "\n",
        "- During prediction, all classifiers are evaluated, and the class with the highest predicted probability is chosen.\n",
        "\n",
        "- Pros:\n",
        "- Simple and fast\n",
        "\n",
        "- Works with all solvers (e.g., liblinear)\n",
        "- Cons:\n",
        "- Doesn’t model competition between classes directly\n",
        "\n",
        "- May be suboptimal when classes are not well-separated\n",
        "\n",
        "-  2. Multinomial Logistic Regression (Softmax Regression)\n",
        "-  How it works:\n",
        "- Uses the softmax function to model the probability distribution over all classes at once.\n",
        "\n",
        "- Directly maximizes the multiclass likelihood.\n",
        "\n",
        "-  Pros:\n",
        "- More accurate when classes are interdependent\n",
        "\n",
        "- Preferred for balanced multiclass problems\n",
        "\n",
        "-  Cons:\n",
        "- Computationally more expensive\n",
        "\n",
        "- Only supported by certain solvers (lbfgs, newton-cg, sag, saga)\n",
        "16. What are the advantages and disadvantages of Logistic Regression?\n",
        "-  Advantages of Logistic Regression\n",
        "- 1. Simple and Interpretable\n",
        "- Coefficients show the effect of features on the probability of the outcome.\n",
        "\n",
        "- Easy to understand and explain.\n",
        "\n",
        "- 2. Efficient and Fast\n",
        "- Computationally inexpensive — works well on small to medium datasets.\n",
        "\n",
        "- Fast to train and predict compared to complex models like SVMs or neural networks.\n",
        "\n",
        "- 3. Probabilistic Output\n",
        "- Provides predicted probabilities, not just class labels.\n",
        "\n",
        "- Useful for decision-making thresholds and ranking.\n",
        "\n",
        "- 4. Works Well with Linearly Separable Data\n",
        "- Performs very well when the data is linearly separable.\n",
        "\n",
        "- 5. Regularization Support\n",
        "- Supports L1, L2, and Elastic Net regularization to prevent overfitting.\n",
        "\n",
        "- 6. Multiclass Support\n",
        "- Can be extended to handle multiclass classification (OvR or multinomial).\n",
        "\n",
        "- 7. Less Prone to Overfitting (with proper regularization)\n",
        "- Especially if number of features is small relative to number of samples.\n",
        "- Disadvantages of Logistic Regression\n",
        "- 1. Assumes Linear Relationship\n",
        "- Assumes a linear relationship between independent variables and the log odds of the outcome.\n",
        "\n",
        "- Can perform poorly with nonlinear or complex interactions unless features are transformed.\n",
        "\n",
        "- 2. Sensitive to Outliers\n",
        "- Outliers can strongly influence the model, especially without regularization.\n",
        "\n",
        "- 3. Not Suitable for High-Dimensional Data (without regularization)\n",
        "- When the number of features is very high (e.g., text data), performance may degrade without feature selection or regularization.\n",
        "\n",
        "- 4. Requires Feature Scaling\n",
        "- Especially important for solvers like sag, saga, or when using regularization.\n",
        "\n",
        "- 5. Limited Expressiveness\n",
        "- Cannot automatically capture nonlinearities, interactions, or complex feature patterns unless engineered.\n",
        "\n",
        "- 6. Not Great for Imbalanced Data\n",
        "- Logistic Regression tends to favor the majority class unless techniques like resampling or class weights are applied.\n",
        "17. What are some use cases of Logistic Regression?\n",
        "- Common Use Cases of Logistic Regression\n",
        "-  1. Medical Diagnosis\n",
        "- Objective: Predict whether a patient has a disease (Yes/No).\n",
        "\n",
        "- Examples:\n",
        "\n",
        "- Predicting diabetes, cancer, or heart disease.\n",
        "\n",
        "- Classifying whether a tumor is malignant or benign.\n",
        "\n",
        "- 2. Credit Scoring & Risk Assessment\n",
        "- Objective: Assess likelihood of loan default.\n",
        "\n",
        "- Examples:\n",
        "\n",
        "- Predict if a customer will repay a loan.\n",
        "\n",
        "- Credit card fraud detection (fraudulent vs. legitimate).\n",
        "\n",
        "-  3. Spam Detection\n",
        "- Objective: Classify emails as spam or not.\n",
        "\n",
        "- Why Logistic Regression: Efficient for high-dimensional text data (with regularization).\n",
        "\n",
        "-  4. Customer Churn Prediction\n",
        "- Objective: Predict if a customer will stop using a service.\n",
        "\n",
        "- Examples:\n",
        "\n",
        "- Telecom or subscription-based services.\n",
        "\n",
        "- Helps in proactive retention strategies.\n",
        "- 5. Click-Through Rate (CTR) Prediction\n",
        "- Objective: Predict if a user will click an ad or link.\n",
        "\n",
        "- Used In: Online advertising, recommender systems.\n",
        "\n",
        "- Logistic regression is often used due to its speed and ability to scale to large datasets.\n",
        "\n",
        "-  6. Admission or Selection Prediction\n",
        "- Objective: Predict binary outcomes like “admitted vs. not admitted” or “hired vs. not hired”.\n",
        "\n",
        "- Examples:\n",
        "\n",
        "- Predicting student admission based on scores and background.\n",
        "\n",
        "-  7. Product Recommendation or Purchase Prediction\n",
        "- Objective: Predict whether a user will buy a product.\n",
        "\n",
        "- Used In: E-commerce platforms.\n",
        "\n",
        "-  8. Risk Modeling in Insurance and Security\n",
        "- Examples:\n",
        "\n",
        "- Predicting claim fraud.\n",
        "\n",
        "- Classifying transaction security breaches.\n",
        "\n",
        "-  9. A/B Testing Analysis\n",
        "- Objective: Evaluate whether a new version of a website or app improves user behavior.\n",
        "\n",
        "- Logistic regression can estimate the likelihood of success (conversion, engagement).\n",
        "\n",
        "- 10. Survey and Behavioral Analysis\n",
        "- Examples:\n",
        "\n",
        "- Predict whether a respondent will agree/disagree with a statement.\n",
        "\n",
        "- Model binary responses in social science research.\n",
        "18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "- 1. Logistic Regression (Binary Classification)\n",
        "- Purpose: Used when there are only two classes (binary classification).\n",
        "\n",
        "- Output: Predicts the probability of one class (typically class 1). The probability of the other class (class 0) is simply 1 minus that.\n",
        "\n",
        "- Activation Function: Sigmoid function is used to map output to a range between 0 and 1.\n",
        "\n",
        "- Equation:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑤\n",
        "𝑇\n",
        "𝑥\n",
        "P(y=1∣x)=\n",
        "1+e\n",
        "−w\n",
        "T\n",
        " x\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "- Loss Function: Binary Cross-Entropy Loss\n",
        "-  2. Softmax Regression (Multinomial Logistic Regression)\n",
        "- Purpose: Used for multiclass classification where there are three or more classes.\n",
        "\n",
        "- Output: Predicts a probability distribution over all classes (each value between 0 and 1, and all values sum to 1).\n",
        "\n",
        "- Activation Function: Softmax function is used to generalize sigmoid to multiple classes.\n",
        "\n",
        "- Equation (for class\n",
        "𝑘\n",
        "k):\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑘\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝑒\n",
        "𝑤\n",
        "𝑘\n",
        "𝑇\n",
        "𝑥\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "𝑒\n",
        "𝑤\n",
        "𝑗\n",
        "𝑇\n",
        "𝑥\n",
        "P(y=k∣x)=\n",
        "∑\n",
        "j=1\n",
        "K\n",
        "​\n",
        " e\n",
        "w\n",
        "j\n",
        "T\n",
        "​\n",
        " x\n",
        "\n",
        "e\n",
        "w\n",
        "k\n",
        "T\n",
        "​\n",
        " x\n",
        "\n",
        "​\n",
        "\n",
        "- where\n",
        "𝐾\n",
        "K is the number of classes.\n",
        "\n",
        "- Loss Function: Categorical Cross-Entropy Loss\n",
        "19.  How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "- Choosing between One-vs-Rest (OvR) and Softmax Regression (Multinomial Logistic Regression) for multiclass classification depends on several factors such as dataset size, model interpretability, performance, and computational complexity.\n",
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "- 1. Basic Idea: What Do Coefficients Represent?\n",
        "In Logistic Regression, each coefficient represents the change in the log-odds of the target variable being 1 (or a specific class) for a one-unit increase in the predictor variable, holding all other variables constant.\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "1\n",
        "−\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "log(\n",
        "1−P(y=1∣x)\n",
        "P(y=1∣x)\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "- 𝛽\n",
        "𝑗\n",
        "β\n",
        "j\n",
        "​\n",
        "  is the coefficient for feature\n",
        "𝑥\n",
        "𝑗\n",
        "x\n",
        "j\n",
        "​\n",
        "\n",
        "\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝛽\n",
        "𝑗\n",
        ")\n",
        "- exp(β\n",
        "j\n",
        "​\n",
        " ) is the odds ratio\n",
        "- 2. Interpretation of a Coefficient\n",
        "𝛽\n",
        "𝑗\n",
        "β\n",
        "j\n",
        "​\n",
        "\n",
        "- If\n",
        "𝛽\n",
        "𝑗\n",
        ">\n",
        "0\n",
        "β\n",
        "j\n",
        "​\n",
        " >0: A one-unit increase in\n",
        "𝑥\n",
        "𝑗\n",
        "x\n",
        "j\n",
        "​\n",
        "  increases the log-odds (and therefore the probability) of the outcome.\n",
        "\n",
        "- If\n",
        "𝛽\n",
        "𝑗\n",
        "<\n",
        "0\n",
        "β\n",
        "j\n",
        "​\n",
        " <0: A one-unit increase in\n",
        "𝑥\n",
        "𝑗\n",
        "x\n",
        "j\n",
        "​\n",
        "  decreases the log-odds of the outcome.\n",
        "\n",
        "- If\n",
        "𝛽\n",
        "𝑗\n",
        "=\n",
        "0\n",
        "β\n",
        "j\n",
        "​\n",
        " =0: No effect on the outcome.\n",
        "- 3. Odds Ratio Interpretation\n",
        "To make interpretation more intuitive, we often exponentiate the coefficient:\n",
        "\n",
        "Odds Ratio\n",
        "=\n",
        "𝑒\n",
        "𝛽\n",
        "𝑗\n",
        "Odds Ratio=e\n",
        "β\n",
        "j\n",
        "​\n",
        "\n",
        "\n",
        "- If\n",
        "𝑒\n",
        "𝛽\n",
        "𝑗\n",
        ">\n",
        "1\n",
        "e\n",
        "β\n",
        "j\n",
        "​\n",
        "\n",
        " >1: The odds increase with\n",
        "𝑥\n",
        "𝑗\n",
        "x\n",
        "j\n",
        "​\n",
        "\n",
        "\n",
        "- If\n",
        "𝑒\n",
        "𝛽\n",
        "𝑗\n",
        "<\n",
        "1\n",
        "e\n",
        "β\n",
        "j\n",
        "​\n",
        "\n",
        " <1: The odds decrease with\n",
        "𝑥\n",
        "𝑗\n",
        "x\n",
        "j\n",
        "​\n",
        "\n",
        "\n",
        "- If\n",
        "𝑒\n",
        "𝛽\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "e\n",
        "β\n",
        "j\n",
        "​\n",
        "\n",
        " =1: No effect\n",
        "-  4. Example\n",
        "- Suppose in a logistic regression model for predicting whether a student passes (1) or fails (0) an exam:\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "(\n",
        "Pass\n",
        ")\n",
        "1\n",
        "−\n",
        "𝑃\n",
        "(\n",
        "Pass\n",
        ")\n",
        ")\n",
        "=\n",
        "−\n",
        "3\n",
        "+\n",
        "0.05\n",
        "×\n",
        "StudyHours\n",
        "log(\n",
        "1−P(Pass)\n",
        "P(Pass)\n",
        "​\n",
        " )=−3+0.05×StudyHours\n",
        "Coefficient for StudyHours = 0.05\n",
        "\n",
        "- Odds Ratio =\n",
        "𝑒\n",
        "0.05\n",
        "≈\n",
        "1.051\n",
        "e\n",
        "0.05\n",
        " ≈1.051\n",
        "\n",
        "- Interpretation:\n",
        "\n",
        "- For every 1-hour increase in study time, the odds of passing increase by about 5.1%, assuming all other factors are constant.\n",
        "- 5. For Categorical Variables\n",
        "- If a categorical feature is one-hot encoded (e.g., Gender: Male vs Female), the coefficient represents the change in log-odds when that category is present, compared to the reference category.\n",
        "\n",
        "-  6. For Multiclass Logistic Regression (Softmax)\n",
        "- Each class (except the reference class) has its own set of coefficients. Coefficients describe how the features influence the log-odds of a given class relative to the reference class.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4yaRMUcIQe1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy.\n",
        "'''\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Apply Logistic Regression\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Print accuracy\n",
        "print(\"Logistic Regression Model Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "'''"
      ],
      "metadata": {
        "id": "O_BrNfcObPmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy.\n",
        "'''\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings related to convergence\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Apply Logistic Regression with L1 regularization\n",
        "model = LogisticRegression(\n",
        "    penalty='l1',         # L1 regularization\n",
        "    solver='saga',        # 'saga' supports L1 for multiclass problems\n",
        "    multi_class='multinomial',\n",
        "    max_iter=200\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Print the accuracy\n",
        "print(\"Logistic Regression with L1 Regularization Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "'''"
      ],
      "metadata": {
        "id": "imz1OiZfbobi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "'''\n",
        " # Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression with L2 regularization (default)\n",
        "model = LogisticRegression(\n",
        "    penalty='l2',              # L2 Regularization\n",
        "    solver='lbfgs',            # Suitable for small-medium datasets\n",
        "    multi_class='multinomial',\n",
        "    max_iter=200\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Print model accuracy\n",
        "print(\"Logistic Regression with L2 Regularization Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "# Step 6: Print model coefficients\n",
        "print(\"\\nModel Coefficients (per class):\")\n",
        "for i, class_label in enumerate(iris.target_names):\n",
        "    print(f\"Class {class_label}: {model.coef_[i]}\")\n",
        " Output Explanation:\n",
        "Accuracy: Shows how well the model performs on unseen test data.\n",
        "\n",
        "Coefficients: Each row in model.coef_ corresponds to a class (since it's multiclass). Each element in the row corresponds to the weight for a feature.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "3dJ-7Bu0b5qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4.  Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "'''\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression with Elastic Net Regularization\n",
        "model = LogisticRegression(\n",
        "    penalty='elasticnet',         # Elastic Net regularization\n",
        "    solver='saga',                # Only 'saga' supports elasticnet\n",
        "    l1_ratio=0.5,                 # Mix between L1 (lasso) and L2 (ridge); 0.5 means 50/50\n",
        "    multi_class='multinomial',   # Multiclass setting\n",
        "    max_iter=500\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Print results\n",
        "print(\"Logistic Regression with Elastic Net Regularization Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "# Step 6: Print model coefficients\n",
        "print(\"\\nModel Coefficients (per class):\")\n",
        "for i, class_label in enumerate(iris.target_names):\n",
        "    print(f\"Class {class_label}: {model.coef_[i]}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "vll8XJPNcSOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr.\n",
        "'''\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load a sample multiclass dataset (Iris)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model with One-vs-Rest strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "# Optional: Print model coefficients\n",
        "print(\"\\nModel Coefficients:\\n\", model.coef_)\n",
        "print(\"Model Intercepts:\\n\", model.intercept_)\n",
        "'''"
      ],
      "metadata": {
        "id": "RPpzFE8achBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy.\n",
        "'''\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Iris for multiclass classification)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "logreg = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Define parameter grid for C and penalty\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit to training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "'''"
      ],
      "metadata": {
        "id": "hg5MdLIReyjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy.\n",
        "'''\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Create Stratified K-Fold cross-validator\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "accuracies = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracies for each fold:\", accuracies)\n",
        "print(\"Average Accuracy:\", np.mean(accuracies))\n",
        "''"
      ],
      "metadata": {
        "id": "tf94yOTlfBtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy.\n",
        "'''\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset from CSV file\n",
        "# Replace 'your_dataset.csv' with your actual file path\n",
        "df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Step 2: Define features and target\n",
        "# Replace 'target_column' with your actual target column name\n",
        "X = df.drop('target_column', axis=1)\n",
        "y = df['target_column']\n",
        "\n",
        "# Step 3: Handle categorical features (if any)\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Step 4: Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 6: Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "'''"
      ],
      "metadata": {
        "id": "4GUmL3mLfOAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy.\n",
        "'''\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load sample dataset (Iris for multiclass classification)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define hyperparameter distributions\n",
        "param_dist = {\n",
        "    'C': np.logspace(-3, 3, 10),  # from 0.001 to 1000\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Randomized search with 10 iterations\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n",
        "                                   n_iter=10, scoring='accuracy', cv=5, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and best cross-validation accuracy\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", random_search.best_score_)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "'''"
      ],
      "metadata": {
        "id": "Esxa-uxgfiER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "- from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model\n",
        "base_model = LogisticRegression(solver='liblinear', max_iter=200)\n",
        "\n",
        "# Wrap it with One-vs-One strategy\n",
        "ovo_model = OneVsOneClassifier(base_model)\n",
        "\n",
        "# Train the model\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"One-vs-One Logistic Regression Accuracy:\", accuracy)\n",
        "'''"
      ],
      "metadata": {
        "id": "pIF4M8Tqf6IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification.\n",
        "'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "\n",
        "# Step 1: Load binary classification dataset (Breast Cancer)\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate and visualize confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Step 7: Plot confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.title('Confusion Matrix - Logistic Regression')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "'''\n"
      ],
      "metadata": {
        "id": "-7KYIG8HgMdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score.\n",
        "'''\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Step 1: Load dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate with precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "\n",
        "# Optional: Full classification report\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "'''"
      ],
      "metadata": {
        "id": "RJdtNeXbgk9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance.\n",
        "'''\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Create imbalanced binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                           weights=[0.9, 0.1],  # 90% of class 0, 10% of class 1\n",
        "                           random_state=42)\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression with class_weight='balanced'\n",
        "model = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Classification Report with class_weight='balanced':\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 5: Confusion matrix visualization\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])\n",
        "plt.title(\"Confusion Matrix (Balanced Class Weights)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "Why class_weight='balanced'?\n",
        "It automatically adjusts weights inversely proportional to class frequencies:\n",
        "\n",
        "weight\n",
        "𝑖\n",
        "=\n",
        "𝑛\n",
        "samples\n",
        "𝑛\n",
        "classes\n",
        "×\n",
        "𝑛\n",
        "𝑖\n",
        "weight\n",
        "i\n",
        "​\n",
        " =\n",
        "n\n",
        "classes\n",
        "​\n",
        " ×n\n",
        "i\n",
        "​\n",
        "\n",
        "n\n",
        "samples\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "This helps the model not bias toward the majority class.\n",
        "Optional: Manually set weights\n",
        "model = LogisticRegression(class_weight={0: 1, 1: 5}, solver='liblinear')\n",
        "'''"
      ],
      "metadata": {
        "id": "wqcEiCqpgyAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance.\n",
        "'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load Titanic dataset from seaborn (you can use your own CSV too)\n",
        "import seaborn as sns\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Step 2: Select features and target\n",
        "df = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "\n",
        "# Step 3: Handle missing values\n",
        "df['age'].fillna(df['age'].median(), inplace=True)\n",
        "df['embarked'].fillna(df['embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Step 4: Encode categorical variables\n",
        "df = pd.get_dummies(df, columns=['sex', 'embarked'], drop_first=True)\n",
        "\n",
        "# Step 5: Split features and target\n",
        "X = df.drop('survived', axis=1)\n",
        "y = df['survived']\n",
        "\n",
        "# Step 6: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 8: Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 9: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 10: Plot Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=['Not Survived', 'Survived'],\n",
        "            yticklabels=['Not Survived', 'Survived'])\n",
        "plt.title(\"Confusion Matrix - Titanic Logistic Regression\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "P11zL0SyhNRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "15.Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling.\n",
        "'''\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load a binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Logistic Regression WITHOUT feature scaling\n",
        "model_no_scaling = LogisticRegression(solver='liblinear')\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Step 4: Apply Standard Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Logistic Regression WITH feature scaling\n",
        "model_with_scaling = LogisticRegression(solver='liblinear')\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 6: Print results\n",
        "print(\"Accuracy WITHOUT Scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy WITH Scaling   :\", accuracy_scaled)\n",
        "\n",
        "# Optional: Suggest interpretation\n",
        "if accuracy_scaled > accuracy_no_scaling:\n",
        "    print(\"\\n✅ Scaling improved model performance.\")\n",
        "elif accuracy_scaled < accuracy_no_scaling:\n",
        "    print(\"\\n⚠️ Model performed better without scaling.\")\n",
        "else:\n",
        "    print(\"\\nℹ️ Scaling had no significant effect on performance.\")\n",
        "'''"
      ],
      "metadata": {
        "id": "34tMQhlOhdtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "16.Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "'''\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "# Step 1: Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict probabilities for ROC-AUC\n",
        "y_proba = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
        "\n",
        "# Step 6: Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n",
        "\n",
        "# Step 7: Plot ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\", color='blue')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - Logistic Regression\")\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "urj4DtJyhuFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy.\n",
        "'''\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load a binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression with custom regularization (C=0.5)\n",
        "# Lower C -> Stronger regularization (like smaller learning rate effect)\n",
        "model = LogisticRegression(C=0.5, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Logistic Regression Accuracy with C=0.5:\", accuracy)\n",
        "'''"
      ],
      "metadata": {
        "id": "MUzMT6Jsh_OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "18. M Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients.\n",
        "'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Extract feature importance\n",
        "coefficients = model.coef_[0]  # Only one row since binary classification\n",
        "feature_importance = pd.Series(coefficients, index=X.columns)\n",
        "feature_importance_sorted = feature_importance.abs().sort_values(ascending=False)\n",
        "\n",
        "# Step 6: Display top features\n",
        "print(\"Top 10 Important Features Based on Coefficients:\\n\")\n",
        "print(feature_importance_sorted.head(10))\n",
        "'''"
      ],
      "metadata": {
        "id": "rN3CtSqeibcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "Score.\n",
        "'''\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Calculate Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "print(\"Cohen's Kappa Score:\", kappa_score)\n",
        "'''"
      ],
      "metadata": {
        "id": "FgRq-zLziqJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classificatio:\n",
        "  '''\n",
        "  import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict probabilities\n",
        "y_scores = model.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
        "\n",
        "# Step 6: Compute Precision-Recall values\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Step 7: Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(recall, precision, color='purple', lw=2, label=f'Avg Precision = {avg_precision:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - Logistic Regression')\n",
        "plt.legend(loc='lower left')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "uDG6Sc1xjDrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accurency.\n",
        "'''\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "accuracy_scores = {}\n",
        "\n",
        "# Train and evaluate Logistic Regression with each solver\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores[solver] = acc\n",
        "    print(f\"Solver: {solver} -> Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(accuracy_scores.keys(), accuracy_scores.values(), color='skyblue')\n",
        "plt.title('Accuracy Comparison of Different Solvers in Logistic Regression')\n",
        "plt.xlabel('Solver')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0.9, 1.0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "f_HoO6g1joeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC).\n",
        "'''\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Step 1: Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict test labels\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate using Matthews Correlation Coefficient\n",
        "mcc_score = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc_score)\n",
        "'''"
      ],
      "metadata": {
        "id": "Q-ECJZudkLMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling.\n",
        "'''\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ---- 1. Logistic Regression on Raw Data ----\n",
        "model_raw = LogisticRegression(max_iter=1000)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "print(f\"Accuracy on Raw Data: {accuracy_raw:.4f}\")\n",
        "\n",
        "# ---- 2. Logistic Regression on Standardized Data ----\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy on Standardized Data: {accuracy_scaled:.4f}\")\n",
        "\n",
        "# ---- Comparison ----\n",
        "improvement = accuracy_scaled - accuracy_raw\n",
        "print(f\"Accuracy Improvement from Scaling: {improvement:.4f}\")\n",
        "Output (Typical for Breast Cancer Dataset):\n",
        "Accuracy on Raw Data: 0.9386\n",
        "Accuracy on Standardized Data: 0.9561\n",
        "Accuracy Improvement from Scaling: 0.0175\n",
        "'''"
      ],
      "metadata": {
        "id": "fp4TBl7XlrOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation.\n",
        "'''\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Define a range of C values to test (log scale)\n",
        "C_values = np.logspace(-4, 4, 10)\n",
        "cv_scores = []\n",
        "\n",
        "# Perform cross-validation for each C\n",
        "for C in C_values:\n",
        "    model = LogisticRegression(C=C, max_iter=1000)\n",
        "    scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
        "    mean_score = scores.mean()\n",
        "    cv_scores.append(mean_score)\n",
        "    print(f\"C={C:.4f}, Accuracy={mean_score:.4f}\")\n",
        "\n",
        "# Find best C\n",
        "best_index = np.argmax(cv_scores)\n",
        "best_C = C_values[best_index]\n",
        "print(f\"\\nBest C value: {best_C:.4f} with cross-validated accuracy: {cv_scores[best_index]:.4f}\")\n",
        "\n",
        "# Plot C vs Accuracy\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(C_values, cv_scores, marker='o', linestyle='-', color='blue')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('C (Inverse of Regularization Strength)')\n",
        "plt.ylabel('Cross-Validated Accuracy')\n",
        "plt.title('Logistic Regression: Accuracy vs. C')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "OUTPUT\n",
        "C=0.0001, Accuracy=0.6270\n",
        "C=0.0010, Accuracy=0.9298\n",
        "C=0.0100, Accuracy=0.9632\n",
        "C=0.1000, Accuracy=0.9666\n",
        "C=1.0000, Accuracy=0.9648\n",
        "C=10.0000, Accuracy=0.9632\n",
        "C=100.0000, Accuracy=0.9632\n",
        "C=1000.0000, Accuracy=0.9632\n",
        "C=10000.0000, Accuracy=0.9632\n",
        "\n",
        "Best C value: 0.1 with cross-validated accuracy: 0.9666\n",
        "'''"
      ],
      "metadata": {
        "id": "6Z9KMQl4mC63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions.\n",
        "'''\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Save the model and scaler\n",
        "joblib.dump(model, 'logistic_model.joblib')\n",
        "joblib.dump(scaler, 'scaler.joblib')\n",
        "print(\"Model and scaler saved successfully.\")\n",
        "\n",
        "# Load the model and scaler\n",
        "loaded_model = joblib.load('logistic_model.joblib')\n",
        "loaded_scaler = joblib.load('scaler.joblib')\n",
        "\n",
        "# Make predictions\n",
        "X_test_scaled_loaded = loaded_scaler.transform(X_test)\n",
        "y_pred = loaded_model.predict(X_test_scaled_loaded)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy using loaded model: {accuracy:.4f}\")\n",
        "OUTPUT\n",
        "Model and scaler saved successfully.\n",
        "Accuracy using loaded model: 0.9561\n"
      ],
      "metadata": {
        "id": "V5R1kublmY3H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}